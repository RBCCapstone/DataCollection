{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Encoding\n",
    "\n",
    "The following script formats articles by features for linear regression (first attempt).  \n",
    "It takes in a list of features and a set of articles, converts to lowercase and creates an encoded matrix (dense)  \n",
    "While this isn't the cleverest method, it provides a usable input for setting up our initial linear regression code.\n",
    "\n",
    "### Limitations:\n",
    "* Proper Nouns should keep their capitals\n",
    "* Punctuation/Stemming etc not incorporated\n",
    "* Bi-grams not accommodated\n",
    "* Could be converted to sparse matrix\n",
    "* No log function incorporated at this point\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyscripter\n",
    "\n",
    "#relevant_nbs = ['FeatureSelection.ipynb']\n",
    "#relevant_nbs = pyscripter.nb_to_py(relevant_nbs)\n",
    "#print(\"y print 2x?\")\n",
    "#print(relevant_nbs)\n",
    "#pyscripter.import_scripts(['FeatureSelection.py'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Testing Word Normalization\n",
    "import nltk\n",
    "\n",
    "## Download Resources\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.stem import *\n",
    "\n",
    "# download required resources\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# we'll compare two stemmers and a lemmatizer\n",
    "lrStem = LancasterStemmer()\n",
    "sbStem = SnowballStemmer(\"english\")\n",
    "prStem = PorterStemmer()\n",
    "wnLemm = WordNetLemmatizer()\n",
    "def wnLemm_v(word):\n",
    "    return WordNetLemmatizer.lemmatize(word, 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(text_col, norm):\n",
    "    DATA_DIR = \"Data\"\n",
    "    feature_filename = norm + text_col + '-FeatureSet.csv'\n",
    "    FEATURES_DIR = os.path.join(DATA_DIR, feature_filename)\n",
    "    ARTICLES_DIR = os.path.join(DATA_DIR, \"Labelled_Articles_.xlsx\")\n",
    "    \n",
    "    fts = pd.read_csv(FEATURES_DIR)\n",
    "    for col in fts.columns:\n",
    "        if not (col.strip() == 'target_group'):\n",
    "            fts = fts.drop([col], axis = 1)\n",
    "    fts.columns = ['index']\n",
    "    fts['index'] = list(map(lambda x: x.strip(), fts['index']))\n",
    "    arts = pd.read_excel(ARTICLES_DIR)\n",
    "    \n",
    "    # Stripping out columns and only passing what we need\n",
    "    artText = arts[text_col]\n",
    "    data = {'fts':fts, 'artText': artText, 'article_id': arts['article_id'], 'market_moving':arts['market_moving']} #**\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binEncoding(data, normalizer=None):\n",
    "    print(\"Binary Encoding\")\n",
    "    fts = data['fts']\n",
    "    artText = data['artText']\n",
    "    df_rows = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    for art in artText:\n",
    "        if type(art) == str: \n",
    "            body = art.lower()\n",
    "            #body = clean_file_text(body)\n",
    "            art_words = tokenizer.tokenize(body)\n",
    "            #insert word normalization\n",
    "            if normalizer:\n",
    "                art_words = [normalizer(word) for word in art_words]\n",
    "            \n",
    "            df_rows.append([1 if word in art_words else 0 for word in fts['index']])\n",
    "        else:\n",
    "            df_rows.append([0 for word in fts['index']])\n",
    "    X = pd.DataFrame(df_rows, columns = fts['index'].values)\n",
    "    print('market_moving' in list(X))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfEncoding(data, normalizer=None):\n",
    "    print(\"tf Encoding\")\n",
    "    fts = data['fts']\n",
    "    artText = data['artText']\n",
    "    \n",
    "    tf_rows = []\n",
    "    for art in artText:\n",
    "        if type(art) == str:\n",
    "            body = art.lower()\n",
    "            body = body.split()\n",
    "            wordsCounter = Counter(body)\n",
    "            tf_rows.append([wordsCounter[word] if word in wordsCounter else 0 for word in fts['index']])\n",
    "        else:\n",
    "            tf_rows.append([0 for word in fts['index']])\n",
    "    X = pd.DataFrame(tf_rows, columns = fts['index'].values)  \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfEncoding(data, normalizer=None):\n",
    "    print(\"tifidf Encoding\")\n",
    "    fts = data['fts']\n",
    "\n",
    "    # Base calculations\n",
    "    binX = binEncoding(data)\n",
    "    tfX = tfEncoding(data)\n",
    "    \n",
    "    # Calculate idf\n",
    "    df_row = [binX[word].sum() for word in fts['index']]\n",
    "    idf = [1/(df+1) for df in df_row]\n",
    "    #transpose list (not the cleverest method)\n",
    "    idf_row = []\n",
    "    idf_row.append(idf)\n",
    "    idf_list = pd.DataFrame(idf_row, columns = fts['index'])\n",
    "    \n",
    "    # Extract term frequencies\n",
    "    tf = tfX.values\n",
    "    # Set up loop to multiply each article (row) by the idf per term (col)\n",
    "    tf_idf = []\n",
    "    r, c = tf.shape\n",
    "    for art in range(0,r):\n",
    "        tf_idf.append(tf[art]*idf)\n",
    "    tf_idf = pd.DataFrame(tf_idf, columns = fts['index'])\n",
    "    X = tf_idf\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(encodeType, text_col=None, norm=None, **kwargs):\n",
    "    # 0 for Binary Encoding\n",
    "    # 1 for Term Frequency Encoding\n",
    "    # 2 for TF-IDF Encoding\n",
    "    # If you'd like to save as csv, use \"csv = True\"\n",
    "        \n",
    "    # Load up data\n",
    "    data = loadData(text_col, norm)\n",
    "    \n",
    "    # Run corresponding encoding type and pass data\n",
    "    options = {0 : binEncoding,\n",
    "                1 : tfEncoding,\n",
    "                2 : tfidfEncoding,}\n",
    "    \n",
    "    if norm:\n",
    "        normalizers = {'lrStem' : lrStem.stem,\n",
    "                       'sbStem' : sbStem.stem,\n",
    "                       'prStem' : prStem.stem,\n",
    "                       'wnLemm' : wnLemm.lemmatize,\n",
    "                       'wnLemm-v':wnLemm_v,\n",
    "                       'baseline':None\n",
    "                      }\n",
    "        normalizer = normalizers[norm]\n",
    "    \n",
    "    X = options[encodeType](data, normalizer)\n",
    "    X = X.drop(['market_moving'], axis = 1) if 'market_moving' in list(X) else X\n",
    "    \n",
    "    # Append Y column and article ids\n",
    "    Y = pd.DataFrame({'article_id':data['article_id'].values, 'market_moving':data['market_moving'].values})\n",
    "    \n",
    "    \n",
    "    XY = Y.join(X)\n",
    "    \n",
    "    # Save as csv file in CLASSIFICATION data folder =)\n",
    "    if ('csv' in kwargs) and (kwargs['csv']):\n",
    "        \n",
    "        # File path for this file\n",
    "        file_name = text_col +'-'+ norm +'-'+  options[encodeType].__name__ + '.csv'\n",
    "        thispath = Path().absolute()\n",
    "        #OUTPUT_DIR = os.path.join(thispath.parent.parent, \"Classification\", \"Data\", file_name)\n",
    "        # if the following line throws an error, use the line after to save in same folder\n",
    "        OUTPUT_DIR = os.path.join(thispath, \"Data\", file_name)\n",
    "        pd.DataFrame.to_csv(XY, path_or_buf=OUTPUT_DIR)\n",
    "        #pd.DataFrame.to_csv(XY, path_or_buf=file_name)\n",
    "    \n",
    "    # Return Panda DataFrame\n",
    "    return XY\n",
    "    \n",
    "\n",
    "\n",
    "def main(): # Stuff to do when run from the command line    \n",
    "    encoding(0, csv = True)\n",
    "    pass  \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testcell\n",
    "\n",
    "#X = encoding(1, csv=True)\n",
    "#X = encoding(2, csv=True)\n",
    "#XY = encoding(0, csv=True)\n",
    "#X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content: baseline\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'Data\\\\baselinecontent-FeatureSet.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-58462f6ae7ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mnrm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'baseline'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxtcol\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnrm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtxtcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnrm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-f16427f80b74>\u001b[0m in \u001b[0;36mencoding\u001b[1;34m(encodeType, text_col, norm, **kwargs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Load up data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Run corresponding encoding type and pass data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-a7ded4ed84e6>\u001b[0m in \u001b[0;36mloadData\u001b[1;34m(text_col, norm)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mARTICLES_DIR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Labelled_Articles_.xlsx\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mfts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFEATURES_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'target_group'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\rbc\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\rbc\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\rbc\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\rbc\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\rbc\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'Data\\\\baselinecontent-FeatureSet.csv' does not exist"
     ]
    }
   ],
   "source": [
    "nrms = ['lrStem', 'sbStem', 'prStem', 'wnLemm', 'wnLemm-v']\n",
    "txtcols = ['content', 'title']\n",
    "\n",
    "for txtcol in txtcols:\n",
    "    #for nrm in nrms:\n",
    "    nrm = 'baseline'\n",
    "    print(txtcol + ': ' + nrm)\n",
    "    encoding(0, text_col = txtcol, norm = nrm, csv=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "duration = 1000  # millisecond\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)\n",
    "winsound.Beep(600, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
